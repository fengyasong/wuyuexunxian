server:
  port: 8000
spring:
  application:
    name: seekfairy
  datasource:
    type: com.alibaba.druid.pool.DruidDataSource
    #druid相关配置
    druid:
      #监控统计拦截的filters
      filter: stat
      #mysql驱动
      driver-class-name: com.mysql.jdbc.Driver
      #基本属性
      url: jdbc:mysql://127.0.0.1:3306/seekfairy?useUnicode=true&characterEncoding=UTF-8&allowMultiQueries=true&serverTimezone=GMT
      username: root
      password: root
      #配置初始化大小/最小/最大
      initial-size: 1
      min-idle: 1
      max-active: 20
      #获取连接等待超时时间
      max-wait: 60000
      #间隔多久进行一次检测，检测需要关闭的空闲连接
      time-between-eviction-runs-millis: 60000
  jackson:
    date-format: yyyy-MM-dd
    time-zone: GMT+8
  #文件上传相关设置
  servlet:
    multipart:
      max-file-size: 50MB
      max-request-size: 100MB
  #spring-aop配置
  aop:
    #启用@Aspectj注解驱动的切面，允许Spring创建基于类的代理
    auto: true
    proxy-target-class: true

mybatis:
  mapper-locations: mapper/**Mapper.xml
  configuration:
    map-underscore-to-camel-case: true
#mybaatis分页插件pagehelper设置
pagehelper:
  pagehelperDialect: mysql
  reasonable: true
  support-methods-arguments: true
  params: count=countSql


logging:
  level:
    #sql日志
    com.aicat.seekfairy.dao: debug
    #root: info

  #here is the importance configs of JWT
jwt:
  route:
    authentication:
      path: /auth/login
  header: Authorization
  expiration: 604800
  secret: com.aicat.seekfairy


#kafka配置信息
spring.kafka:
  producer:
    bootstrap-servers: 128.196.1.45:9092
    batch-size: 16785                                   #一次最多发送数据量
    retries: 1                                          #发送失败后的重复发送次数
    buffer-memory: 33554432                             #32M批处理缓冲区
    linger: 1
    properties.max.requst.size: 2097152
    key-serializer: org.apache.kafka.common.serialization.StringSerializer
    value-serializer: org.apache.kafka.common.serialization.StringSerializer
  consumer:
    bootstrap-servers: 128.196.1.45:9092
    group-id: group0 #设置一个默认组
    auto-offset-reset: latest                           #最早未被消费的offset earliest
    max-poll-records: 3100                              #批量消费一次最大拉取的数据量
    enable-auto-commit: false                           #是否开启自动提交
    auto-commit-interval: 1000                          #自动提交的间隔时间
    session-timeout: 20000                              #连接超时时间
    max-poll-interval: 15000                            #手动提交设置与poll的心跳数,如果消息队列中没有消息，等待毫秒后，调用poll()方法。如果队列中有消息，立即消费消息，每次消费的消息的多少可以通过max.poll.records配置。
    max-partition-fetch-bytes: 15728640                 #设置拉取数据的大小,15M
    #set comsumer max fetch.byte 2*1024*1024
    properties.max.partition.fetch.bytes: 2097152
    #key-value序列化反序列化
    key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
    value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
  listener:
    batch-listener: true                                #是否开启批量消费，true表示批量消费
    concurrencys: 2,4                                     #设置消费的线程数
    poll-timeout: 1500                                  #只限自动提交，
    topics: TranLog,test
